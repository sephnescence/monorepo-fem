# Reusable workflow for deploying Lambda functions via AWS SAM
#
# This workflow handles the common deployment pattern for our Lambda applications.
# It expects build artefacts to already exist (uploaded by the calling workflow).
#
# Usage example:
#   deploy:
#     uses: ./.github/workflows/reusable-deploy-lambda.yml
#     with:
#       app-name: pulse-publisher
#       environment: dev
#       working-directory: apps/pulse-publisher
#       output-key: PulsePublisherFunction
#       artefact-name: pulse-build-artefacts
#     secrets:
#       aws-oidc-deploy-role-arn: ${{ secrets.AWS_OIDC_DEPLOY_ROLE_ARN }}
#
# Note: This workflow declares the environment block internally, which enables
# environment protection and approvals (if configured in GitHub Settings).
# Secrets are passed explicitly from the calling workflow for transparency.

name: Reusable Lambda Deployment

on:
  workflow_call:
    inputs:
      app-name:
        description: 'Name of the app to deploy (e.g., pulse-publisher, heartbeat-publisher)'
        required: true
        type: string
      environment:
        description: 'Target environment (dev/exp/prod)'
        required: true
        type: string
      working-directory:
        description: 'Working directory for the app (e.g., apps/pulse-publisher)'
        required: true
        type: string
      output-key:
        description: 'CloudFormation output key for function name (e.g., PulsePublisherFunction)'
        required: true
        type: string
      artefact-name:
        description: 'Name of the build artefact to download (e.g., pulse-build-artefacts)'
        required: true
        type: string
      policy-file:
        description: 'Path to the IAM policy file for this app (e.g., .github/policies/heartbeat-publisher-deploy-policy.json)'
        required: true
        type: string
    secrets:
      aws-oidc-deploy-role-arn:
        description: 'AWS OIDC role ARN for deployment authentication'
        required: true
      aws-oidc-policy-manager-role-arn:
        description: 'AWS OIDC role ARN for policy management (read IAM policies)'
        required: true

permissions:
  id-token: write
  contents: read

jobs:
  deploy:
    name: Deploy ${{ inputs.app-name }} to ${{ inputs.environment }}
    runs-on: ubuntu-latest
    # Environment block enables environment protection and secret resolution
    # Repository-level secrets are accessible from within environment-protected jobs
    environment:
      name: ${{ inputs.environment }}
      url: https://ap-southeast-2.console.aws.amazon.com/cloudformation/home?region=ap-southeast-2#/stacks
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Download build artefacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.artefact-name }}
          path: ${{ inputs.working-directory }}/dist/

      - name: Debug - Verify artifact structure
        run: |
          echo "=== Workspace root structure ==="
          ls -la
          echo ""
          echo "=== Apps directory ==="
          ls -la apps/ || echo "apps/ does not exist"
          echo ""
          echo "=== Working directory contents ==="
          ls -laR ${{ inputs.working-directory }}
          echo ""
          echo "=== Looking for .mjs and .js files ==="
          find . -type f \( -name "*.mjs" -o -name "*.js" \) | head -20

      # Authentication happens via OpenID Connect (OIDC)
      # The role ARN is explicitly passed from the calling workflow
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.aws-oidc-deploy-role-arn }}
          aws-region: ap-southeast-2

      - name: Validate IAM policy drift
        id: policy-validation
        continue-on-error: true
        run: |
          set +e  # Don't exit on error - we want to report but not fail

          ENV="${{ inputs.environment }}"
          POLICY_FILE="${{ inputs.policy-file }}"
          APP_NAME="${{ inputs.app-name }}"
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          AWS_REGION="ap-southeast-2"

          echo "=== Policy Drift Validation ==="
          echo "Environment: $ENV"
          echo "App: $APP_NAME"
          echo "Policy file: $POLICY_FILE"
          echo ""

          # Check if policy file exists
          if [ ! -f "$POLICY_FILE" ]; then
            echo "❌ ERROR: Policy file not found: $POLICY_FILE"
            echo "Policy validation requires a policy file in the repository."
            echo "policy-validation-failed=true" >> $GITHUB_OUTPUT
            if [ "$ENV" == "prod" ]; then
              echo "::error::Policy file missing for production deployment"
              exit 1
            else
              echo "::warning::Policy file missing - skipping policy validation"
              exit 0
            fi
          fi

          # Assume policy manager role to read IAM policies
          echo "Assuming PolicyManager role to read deployed policies..."
          POLICY_MANAGER_ROLE="${{ secrets.aws-oidc-policy-manager-role-arn }}"

          if [ -z "$POLICY_MANAGER_ROLE" ]; then
            echo "⚠️  WARNING: PolicyManager role ARN not configured"
            echo "Skipping policy validation. To enable:"
            echo "  - Add secret: AWS_OIDC_POLICY_MANAGER_ROLE_ARN__MONOREPO_FEM__${ENV^^}"
            echo "::warning::PolicyManager role not configured - skipping policy validation"
            exit 0
          fi

          # Get deployment role name from the deploy role ARN
          DEPLOY_ROLE_ARN="${{ secrets.aws-oidc-deploy-role-arn }}"
          DEPLOY_ROLE_NAME=$(echo "$DEPLOY_ROLE_ARN" | sed 's|.*/||')

          echo "Deployment role: $DEPLOY_ROLE_NAME"
          echo ""

          # Assume policy manager role
          TEMP_CREDS=$(aws sts assume-role \
            --role-arn "$POLICY_MANAGER_ROLE" \
            --role-session-name "policy-validation-$APP_NAME-$ENV" \
            --duration-seconds 900 \
            --output json)

          if [ $? -ne 0 ]; then
            echo "⚠️  WARNING: Failed to assume PolicyManager role"
            echo "Skipping policy validation."
            echo "::warning::Failed to assume PolicyManager role - skipping policy validation"
            exit 0
          fi

          # Export temporary credentials
          export AWS_ACCESS_KEY_ID=$(echo "$TEMP_CREDS" | jq -r .Credentials.AccessKeyId)
          export AWS_SECRET_ACCESS_KEY=$(echo "$TEMP_CREDS" | jq -r .Credentials.SecretAccessKey)
          export AWS_SESSION_TOKEN=$(echo "$TEMP_CREDS" | jq -r .Credentials.SessionToken)

          # Get inline policy from deployment role
          echo "Retrieving deployed policy for role: $DEPLOY_ROLE_NAME"
          POLICY_NAMES=$(aws iam list-role-policies \
            --role-name "$DEPLOY_ROLE_NAME" \
            --query 'PolicyNames[0]' \
            --output text)

          if [ -z "$POLICY_NAMES" ] || [ "$POLICY_NAMES" == "None" ]; then
            echo "⚠️  WARNING: No inline policies found on deployment role"
            echo "The deployment role may not have been created yet."
            echo "::warning::No inline policies found - this may be the first deployment"
            exit 0
          fi

          # Get the policy document
          aws iam get-role-policy \
            --role-name "$DEPLOY_ROLE_NAME" \
            --policy-name "$POLICY_NAMES" \
            --query 'PolicyDocument' \
            --output json > /tmp/deployed-policy-raw.json

          if [ $? -ne 0 ]; then
            echo "⚠️  WARNING: Failed to retrieve deployed policy"
            echo "::warning::Failed to retrieve deployed policy"
            exit 0
          fi

          echo "✓ Retrieved deployed policy"
          echo ""

          # Normalise both policies
          echo "Normalising policies for comparison..."

          # Normalise repository policy
          .github/scripts/normalise-policy.sh \
            "$POLICY_FILE" \
            "$AWS_ACCOUNT_ID" \
            "$AWS_REGION" \
            "$ENV" \
            > /tmp/repository-policy-normalised.json

          # Normalise deployed policy (already has substitutions)
          cat /tmp/deployed-policy-raw.json | jq --sort-keys '.' > /tmp/deployed-policy-normalised.json

          echo "✓ Policies normalised"
          echo ""

          # Compare policies
          echo "Comparing deployed policy with repository policy..."
          .github/scripts/compare-policies.sh \
            /tmp/deployed-policy-normalised.json \
            /tmp/repository-policy-normalised.json

          COMPARISON_RESULT=$?

          # Check if there were differences based on the output
          if grep -q "Policy differences detected" /tmp/comparison-output.txt 2>/dev/null; then
            echo "policy-drift-detected=true" >> $GITHUB_OUTPUT
            if [ "$ENV" == "prod" ]; then
              echo "::error::Policy drift detected in production environment"
              echo "policy-validation-failed=true" >> $GITHUB_OUTPUT
              exit 1
            else
              echo "::warning::Policy drift detected in $ENV environment"
            fi
          else
            echo "policy-drift-detected=false" >> $GITHUB_OUTPUT
          fi

          exit 0

      - name: Check PolicyManager role exists
        id: policy-manager-check
        run: |
          ENV="${{ inputs.environment }}"
          ROLE_NAME="monorepo-fem-policy-manager-$ENV"
          STACK_NAME="monorepo-fem-devops-$ENV"

          if ! aws iam get-role --role-name "$ROLE_NAME" 2>/dev/null; then
            echo "⚠️  WARNING: PolicyManager role does not exist for $ENV environment"
            echo "The DevOps infrastructure must be bootstrapped before deploying applications."
            echo ""
            echo "To bootstrap the $ENV environment, a trusted person with AWS admin access must run:"
            echo ""
            echo "  aws cloudformation create-stack \\"
            echo "    --stack-name $STACK_NAME \\"
            echo "    --template-body file://devops/$ENV/monorepo-fem-github-actions-sam-deploy-$ENV.yml \\"
            echo "    --capabilities CAPABILITY_NAMED_IAM \\"
            echo "    --region ap-southeast-2 \\"
            echo "    --parameters \\"
            echo "      ParameterKey=GitHubOrganization,ParameterValue=sephnescence \\"
            echo "      ParameterKey=GitHubRepository,ParameterValue=monorepo-fem \\"
            echo "      ParameterKey=Environment,ParameterValue=$ENV \\"
            echo "      ParameterKey=DeploymentBranch,ParameterValue=deploy-$ENV \\"
            echo "      ParameterKey=AWSAccountId,ParameterValue=395380602678 \\"
            echo "      ParameterKey=AWSRegion,ParameterValue=ap-southeast-2"
            echo ""
            echo "For detailed instructions, see: devops/README.md"
            echo ""
            echo "policy-manager-exists=false" >> $GITHUB_OUTPUT
            exit 0
          else
            echo "✓ PolicyManager role exists: $ROLE_NAME"
            echo "policy-manager-exists=true" >> $GITHUB_OUTPUT
          fi

      - name: Exit if PolicyManager missing
        if: steps.policy-manager-check.outputs.policy-manager-exists == 'false'
        run: |
          echo "::warning::Deployment skipped - PolicyManager role not found. See bootstrap instructions above."
          exit 0

      - name: Validate SAM template
        working-directory: ${{ inputs.working-directory }}
        run: sam validate --lint

      - name: Check stack exists
        working-directory: ${{ inputs.working-directory }}
        run: |
          ENV="${{ inputs.environment }}"
          STACK_NAME="${{ inputs.app-name }}-$ENV"
          if ! aws cloudformation describe-stacks --stack-name $STACK_NAME 2>/dev/null; then
            echo "⚠️  WARNING: Stack does not exist. This is a CREATE operation."
            echo "If deployment fails, stack will enter ROLLBACK_COMPLETE state."
            echo "SAM will auto-delete the stack on failure to prevent stuck state."
          else
            echo "✓ Stack exists. This is an UPDATE operation."
          fi

      - name: SAM Build
        working-directory: ${{ inputs.working-directory }}
        run: sam build

      - name: SAM Deploy
        working-directory: ${{ inputs.working-directory }}
        run: |
          ENV="${{ inputs.environment }}"
          STACK_NAME="${{ inputs.app-name }}-$ENV"
          sam deploy \
            --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
            --config-env $ENV \
            --no-confirm-changeset \
            --no-fail-on-empty-changeset \
            --on-failure ROLLBACK \
            --parameter-overrides Environment=$ENV \
            --region ap-southeast-2 \
            --stack-name $STACK_NAME \
            --s3-prefix $STACK_NAME \
            --resolve-s3 true

      - name: Get deployment info
        id: deployment-info
        working-directory: ${{ inputs.working-directory }}
        run: |
          ENV="${{ inputs.environment }}"
          STACK_NAME="${{ inputs.app-name }}-$ENV"
          FUNCTION_NAME=$(aws cloudformation describe-stacks \
            --stack-name $STACK_NAME \
            --query "Stacks[0].Outputs[?OutputKey=='${{ inputs.output-key }}'].OutputValue" \
            --output text)

          if [ -z "$FUNCTION_NAME" ]; then
            echo "❌ ERROR: Failed to retrieve function name from stack outputs"
            echo "Stack: $STACK_NAME"
            echo "Output key: ${{ inputs.output-key }}"
            exit 1
          fi

          echo "function-name=$FUNCTION_NAME" >> $GITHUB_OUTPUT
          echo "stack-name=$STACK_NAME" >> $GITHUB_OUTPUT
          echo "✓ Retrieved function name: $FUNCTION_NAME"

      - name: Health check - Invoke Lambda
        run: |
          RESPONSE=$(aws lambda invoke \
            --function-name ${{ steps.deployment-info.outputs.function-name }} \
            --payload '{}' \
            --cli-binary-format raw-in-base64-out \
            /tmp/response.json)

          STATUS_CODE=$(echo $RESPONSE | jq -r '.StatusCode')
          if [ "$STATUS_CODE" != "200" ]; then
            echo "Lambda invocation failed with status code: $STATUS_CODE"
            exit 1
          fi

          echo "Lambda invoked successfully"
          cat /tmp/response.json

      - name: Monitor CloudWatch Alarms
        run: |
          echo "Waiting 2 minutes for CloudWatch metrics to populate..."
          sleep 120

          STACK_NAME="${{ steps.deployment-info.outputs.stack-name }}"
          ALARMS=$(aws cloudwatch describe-alarms \
            --alarm-name-prefix "${STACK_NAME}" \
            --state-value ALARM \
            --query 'MetricAlarms[*].[AlarmName,StateReason]' \
            --output text)

          if [ -n "$ALARMS" ]; then
            echo "CloudWatch alarms in ALARM state detected:"
            echo "$ALARMS"
            echo "Deployment may have issues. Check CloudWatch console."
            exit 1
          fi

          echo "No alarms triggered. Deployment successful."

      - name: Clean up on failure
        if: failure()
        working-directory: ${{ inputs.working-directory }}
        run: |
          ENV="${{ inputs.environment }}"
          STACK_NAME="${{ inputs.app-name }}-$ENV"

          # Get current stack status
          STACK_STATUS=$(aws cloudformation describe-stacks \
            --stack-name $STACK_NAME \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null || echo "DOES_NOT_EXIST")

          echo "Stack status: $STACK_STATUS"

          if [[ "$STACK_STATUS" == "ROLLBACK_COMPLETE" ]]; then
            echo "⚠️  Stack in ROLLBACK_COMPLETE state. Deleting stack to enable future deployments..."
            aws cloudformation delete-stack --stack-name $STACK_NAME
            echo "Stack deletion initiated. Future deployments will recreate the stack."
          elif [[ "$STACK_STATUS" =~ ^(UPDATE_FAILED|UPDATE_ROLLBACK_FAILED)$ ]]; then
            echo "Update failed. Attempting to rollback to previous version..."
            aws cloudformation rollback-stack --stack-name $STACK_NAME || echo "Rollback command failed, but stack may auto-recover"
          elif [[ "$STACK_STATUS" == "DOES_NOT_EXIST" ]]; then
            echo "Stack does not exist (CREATE operation failed). SAM should have auto-cleaned it."
          else
            echo "Stack in state: $STACK_STATUS. Manual intervention may be required."
          fi
